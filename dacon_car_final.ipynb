{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import TrivialAugmentWide, RandomErasing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import timm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() and torch.cuda.device_count() > 1 else \"cuda:0\")\n",
    "\n",
    "print(\"사용 중인 디바이스:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE': 448,               \n",
    "    'CONVNEXT_IMG_SIZE': 448,     \n",
    "    'EVA_IMG_SIZE': 336,          \n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPOCHS': 100,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'SEED': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_test=False):\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "\n",
    "        if is_test:\n",
    "            self.samples = sorted([\n",
    "                os.path.join(root_dir, f) for f in os.listdir(root_dir)\n",
    "                if f.lower().endswith('.jpg')\n",
    "            ])\n",
    "        else:\n",
    "            self.classes = sorted(os.listdir(root_dir))\n",
    "            self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "            for cls in self.classes:\n",
    "                for f in os.listdir(os.path.join(root_dir, cls)):\n",
    "                    if f.lower().endswith('.jpg'):\n",
    "                        self.samples.append((os.path.join(root_dir, cls, f), self.class_to_idx[cls]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            img = Image.open(self.samples[idx]).convert('RGB')\n",
    "            return self.transform(img) if self.transform else img\n",
    "        else:\n",
    "            img_path, label = self.samples[idx]\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            return (self.transform(img) if self.transform else img, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = './train'\n",
    "test_root = './test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomHalfCrop:\n",
    "    def __init__(self, p=0.3):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        if random.random() < self.p and w > 2 and h > 2:\n",
    "            side = random.choice(['left', 'right', 'top', 'bottom'])\n",
    "            if side == 'left' and w // 2 > 1:\n",
    "                img = img.crop((0, 0, w // 2, h))\n",
    "            elif side == 'right' and w // 2 > 1:\n",
    "                img = img.crop((w // 2, 0, w, h))\n",
    "            elif side == 'top' and h // 2 > 1:\n",
    "                img = img.crop((0, 0, w, h // 2))\n",
    "            elif side == 'bottom' and h // 2 > 1:\n",
    "                img = img.crop((0, h // 2, w, h))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_train_transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),\n",
    "    RandomHalfCrop(p=0.3),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=15,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=(0.9, 1.1),\n",
    "        shear=10,\n",
    "        interpolation=transforms.InterpolationMode.BILINEAR,\n",
    "        fill=(124, 117, 104)\n",
    "    ),\n",
    "    TrivialAugmentWide(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.1), ratio=(0.3, 3.3), value='random')\n",
    "])\n",
    "\n",
    "conv_val_transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "eva_train_transform = transforms.Compose([\n",
    "    transforms.Resize((336, 336)),\n",
    "    RandomHalfCrop(p=0.3),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=15,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=(0.9, 1.1),\n",
    "        shear=10,\n",
    "        interpolation=transforms.InterpolationMode.BILINEAR,\n",
    "        fill=(124, 117, 104)\n",
    "    ),\n",
    "    TrivialAugmentWide(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.1), ratio=(0.3, 3.3), value='random')\n",
    "])\n",
    "\n",
    "eva_val_transform = transforms.Compose([\n",
    "    transforms.Resize((336, 336)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = CustomImageDataset(train_root, transform=None)\n",
    "print(f\"총 이미지 수: {len(full_dataset)}\")\n",
    "\n",
    "targets = [label for _, label in full_dataset.samples]\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(targets)), test_size=0.2, stratify=targets, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ ConvNeXtV2용\n",
    "conv_train_dataset = Subset(CustomImageDataset(train_root, transform=conv_train_transform), train_idx)\n",
    "conv_val_dataset = Subset(CustomImageDataset(train_root, transform=conv_val_transform), val_idx)\n",
    "\n",
    "# ✅ EVA용\n",
    "eva_train_dataset = Subset(CustomImageDataset(train_root, transform=eva_train_transform), train_idx)\n",
    "eva_val_dataset = Subset(CustomImageDataset(train_root, transform=eva_val_transform), val_idx)\n",
    "\n",
    "print(f\"[ConvNeXt] train 이미지 수: {len(conv_train_dataset)}, valid 이미지 수: {len(conv_val_dataset)}\")\n",
    "print(f\"[EVA] train 이미지 수: {len(eva_train_dataset)}, valid 이미지 수: {len(eva_val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            'hf_hub:timm/eva_large_patch14_336.in22k_ft_in1k',\n",
    "            pretrained=True,\n",
    "            num_classes=0  \n",
    "        )\n",
    "        self.head = nn.Linear(self.backbone.num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            'hf_hub:timm/convnextv2_base.fcmae_ft_in22k_in1k_384',\n",
    "            pretrained=True,\n",
    "            num_classes=0\n",
    "        )\n",
    "        self.feature_dim = self.backbone.num_features\n",
    "        self.head = nn.Linear(self.feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
    "    \"\"\"\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        first_cycle_steps (int): First cycle step size.\n",
    "        cycle_mult(float): Cycle steps magnification. Default: -1.\n",
    "        max_lr(float): First cycle's max learning rate. Default: 0.1.\n",
    "        min_lr(float): Min learning rate. Default: 0.001.\n",
    "        warmup_steps(int): Linear warmup step size. Default: 0.\n",
    "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 optimizer : torch.optim.Optimizer,\n",
    "                 first_cycle_steps : int,\n",
    "                 cycle_mult : float = 1.,\n",
    "                 max_lr : float = 0.1,\n",
    "                 min_lr : float = 0.001,\n",
    "                 warmup_steps : int = 0,\n",
    "                 gamma : float = 1.,\n",
    "                 last_epoch : int = -1\n",
    "        ):\n",
    "        assert warmup_steps < first_cycle_steps\n",
    "        \n",
    "        self.first_cycle_steps = first_cycle_steps \n",
    "        self.cycle_mult = cycle_mult \n",
    "        self.base_max_lr = max_lr \n",
    "        self.max_lr = max_lr \n",
    "        self.min_lr = min_lr \n",
    "        self.warmup_steps = warmup_steps \n",
    "        self.gamma = gamma \n",
    "        \n",
    "        self.cur_cycle_steps = first_cycle_steps\n",
    "        self.cycle = 0 \n",
    "        self.step_in_cycle = last_epoch \n",
    "        \n",
    "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "        \n",
    "        self.init_lr()\n",
    "    \n",
    "    def init_lr(self):\n",
    "        self.base_lrs = []\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.min_lr\n",
    "            self.base_lrs.append(self.min_lr)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.step_in_cycle == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.step_in_cycle < self.warmup_steps:\n",
    "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.max_lr - base_lr) \\\n",
    "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\n",
    "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.step_in_cycle = self.step_in_cycle + 1\n",
    "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
    "                self.cycle += 1\n",
    "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
    "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
    "        else:\n",
    "            if epoch >= self.first_cycle_steps:\n",
    "                if self.cycle_mult == 1.:\n",
    "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
    "                    self.cycle = epoch // self.first_cycle_steps\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
    "                    self.cycle = n\n",
    "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
    "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
    "            else:\n",
    "                self.cur_cycle_steps = self.first_cycle_steps\n",
    "                self.step_in_cycle = epoch\n",
    "                \n",
    "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_configs = [\n",
    "    (\"ConvNeXtV2\", BaseModel, CFG['CONVNEXT_IMG_SIZE'], \"conv\"),\n",
    "    (\"EVA\", EvaModel, CFG['EVA_IMG_SIZE'], \"eva\")\n",
    "]\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() and torch.cuda.device_count() > 1 else \"cuda:0\")\n",
    "\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=CFG['SEED'])\n",
    "\n",
    "\n",
    "targets = [label for _, label in full_dataset.samples]\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "for model_name, model_class, img_size, model_prefix in model_configs:\n",
    "    print(f\"\\n\\U0001f9e0 Start training for {model_name}...\")\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(targets)), targets)):\n",
    "        print(f\"\\n\\U0001f501 Fold {fold+1}/{NUM_FOLDS} - {model_name}\")\n",
    "\n",
    "        \n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        \n",
    "        train_dataset = Subset(CustomImageDataset(train_root, transform=train_transform), train_idx)\n",
    "        val_dataset = Subset(CustomImageDataset(train_root, transform=val_transform), val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        \n",
    "        model = model_class(num_classes=len(class_names)).to(device)\n",
    "        ema = ExponentialMovingAverage(model.parameters(), decay=0.9995)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['LEARNING_RATE'], weight_decay=1e-4)\n",
    "        scheduler = CosineAnnealingWarmupRestarts(\n",
    "            optimizer, first_cycle_steps=10, cycle_mult=2,\n",
    "            max_lr=CFG['LEARNING_RATE'], min_lr=1e-6,\n",
    "            warmup_steps=3, gamma=0.5\n",
    "        )\n",
    "        scaler = GradScaler()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        best_logloss = float('inf')\n",
    "        counter = 0\n",
    "        patience = 5\n",
    "\n",
    "        \n",
    "        for epoch in range(CFG['EPOCHS']):\n",
    "            model.train()\n",
    "            total_train_loss = 0.0\n",
    "\n",
    "            for images, labels in tqdm(train_loader, desc=f\"[{model_name} Fold {fold+1}] Epoch {epoch+1}\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with autocast(device_type='cuda'):\n",
    "                    outputs = model(images)\n",
    "                    probs = F.softmax(outputs, dim=1)\n",
    "                    true_probs = probs[torch.arange(len(labels)), labels]\n",
    "                    weights = (1.0 - true_probs).detach()\n",
    "                    log_probs = F.log_softmax(outputs, dim=1)\n",
    "                    loss_per_sample = F.nll_loss(log_probs, labels, reduction='none')\n",
    "                    loss = (weights * loss_per_sample).mean()\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                ema.update()\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "            \n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            all_probs = []\n",
    "            all_labels = []\n",
    "\n",
    "            with ema.average_parameters(), torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                with autocast(device_type='cuda'):\n",
    "                    outputs = model(images)                 \n",
    "                    loss = criterion(outputs, labels)       \n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    probs = F.softmax(outputs, dim=1)\n",
    "                    all_probs.append(probs.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    correct += (preds == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "\n",
    "            all_probs = np.vstack(all_probs)\n",
    "            try:\n",
    "                val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n",
    "            except:\n",
    "                val_logloss = float('inf')\n",
    "\n",
    "            val_acc = 100 * correct / total\n",
    "\n",
    "            print(f\"\\n📊 Epoch {epoch+1} Summary - Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Acc: {val_acc:.2f}% | LogLoss: {val_logloss:.4f}\")\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            if val_logloss < best_logloss:\n",
    "                best_logloss = val_logloss\n",
    "                torch.save(model.state_dict(), f\"{model_prefix}_fold{fold+1}_best.pth\")\n",
    "                print(f\"✅ Best model saved: {model_prefix}_fold{fold+1}_best.pth\")\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(\"🛑 Early stopping\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_FOLDS = 5\n",
    "BATCH_SIZE = CFG['BATCH_SIZE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_probs(model_class, fold_paths, loader, model_name):\n",
    "    all_fold_probs = []\n",
    "    with torch.no_grad():\n",
    "        for fold, path in enumerate(fold_paths):\n",
    "            print(f\"📦 {model_name} Fold {fold+1} → {path}\")\n",
    "            model = model_class(num_classes=len(class_names)).to(device)\n",
    "            model.load_state_dict(torch.load(path, map_location=device))\n",
    "            model.eval()\n",
    "\n",
    "            fold_probs = []\n",
    "            for images in tqdm(loader, desc=f\"{model_name} - Fold {fold+1}\", leave=False):\n",
    "                images = images.to(device)\n",
    "                outputs = model(images)\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                fold_probs.append(probs.cpu())\n",
    "\n",
    "            fold_probs = torch.cat(fold_probs, dim=0)\n",
    "            all_fold_probs.append(fold_probs)\n",
    "\n",
    "    return torch.stack(all_fold_probs).mean(dim=0).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_transform = transforms.Compose([\n",
    "    transforms.Resize((CFG['EVA_IMG_SIZE'], CFG['EVA_IMG_SIZE'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "conv_transform = transforms.Compose([\n",
    "    transforms.Resize((CFG['CONVNEXT_IMG_SIZE'], CFG['CONVNEXT_IMG_SIZE'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "eva_dataset = CustomImageDataset(test_root, transform=eva_transform, is_test=True)\n",
    "conv_dataset = CustomImageDataset(test_root, transform=conv_transform, is_test=True)\n",
    "\n",
    "eva_loader = DataLoader(eva_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=4, pin_memory=True)\n",
    "conv_loader = DataLoader(conv_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_fold_paths = [f\"eva_fold{fold+1}_best.pth\" for fold in range(NUM_FOLDS)]\n",
    "conv_fold_paths = [f\"conv_fold{fold+1}_best.pth\" for fold in range(NUM_FOLDS)]\n",
    "\n",
    "print(\"🚀 EVA 추론 시작\")\n",
    "eva_preds = infer_probs(EvaModel, eva_fold_paths, eva_loader, model_name=\"EVA\")\n",
    "\n",
    "print(\"🚀 ConvNeXtV2 추론 시작\")\n",
    "conv_preds = infer_probs(BaseModel, conv_fold_paths, conv_loader, model_name=\"ConvNeXtV2\")\n",
    "\n",
    "ensemble_preds = (eva_preds + conv_preds) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fold_ensemble(model_class, fold_paths, loader, desc):\n",
    "    all_fold_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for fold, path in enumerate(fold_paths):\n",
    "            print(f\"{desc} Fold {fold+1} → {path}\")\n",
    "            model = model_class(num_classes=len(class_names)).to(device)\n",
    "            model.load_state_dict(torch.load(path, map_location=device))\n",
    "            model.eval()\n",
    "\n",
    "            fold_probs = []\n",
    "            for images in tqdm(loader, desc=f\"{desc} - Fold {fold+1}\", leave=False):\n",
    "                images = images.to(device)\n",
    "                outputs = model(images)\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                fold_probs.append(probs.cpu())\n",
    "\n",
    "            fold_probs = torch.cat(fold_probs, dim=0)\n",
    "            all_fold_probs.append(fold_probs)\n",
    "\n",
    "    return torch.stack(all_fold_probs).mean(dim=0).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"sample_submission.csv\", encoding=\"utf-8-sig\")\n",
    "class_columns = submission.columns[1:]\n",
    "pred_df = pd.DataFrame(ensemble_preds, columns=class_names)\n",
    "submission[class_columns] = pred_df[class_columns].values\n",
    "submission.to_csv(\"ensemble_eva_convnext_kfold.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"✅ 최종 앙상블 저장 완료: ensemble_eva_convnext_kfold.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon_dong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
